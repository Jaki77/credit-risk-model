# ğŸ¦ Credit Risk Probability Model for Alternative Data

[![CI/CD Pipeline](https://github.com/Jaki77/credit-risk-model/actions/workflows/ci.yml/badge.svg)](https://github.com/Jaki77/credit-risk-model/actions/workflows/ci.yml)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code Coverage](https://img.shields.io/badge/coverage-87%25-brightgreen.svg)](https://github.com/Jaki77/credit-risk-model/actions)
[![Streamlit](https://img.shields.io/badge/dashboard-streamlit-ff4b4b.svg)](https://streamlit.io)
[![SHAP](https://img.shields.io/badge/explainability-SHAP-blue.svg)](https://github.com/slundberg/shap)

---

## ğŸ“‹ Table of Contents
- [Business Problem](#-business-problem)
- [Solution Overview](#-solution-overview)
- [Key Results](#-key-results)
- [Demo](#-demo)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Technical Deep Dive](#-technical-deep-dive)
- [Model Explainability](#-model-explainability)
- [Business Impact](#-business-impact)
- [Future Improvements](#-future-improvements)
- [Author](#-author)

---

## ğŸ’¼ Business Problem

**Bati Bank**, a leading financial service provider with 10+ years of experience, is launching a **"Buy Now, Pay Later" (BNPL)** service in partnership with a successful eCommerce platform. The challenge? **Traditional credit scoring requires historical loan repayment data**â€”which doesn't exist for these new customers.

**The Pain Point:**
- Bank cannot assess creditworthiness without loan history
- eCommerce platform has rich transaction data but no credit expertise
- Customers risk being excluded from credit access
- Bank faces potential losses from undetected high-risk borrowers

**Why This Matters:**
> In Ethiopia's growing digital economy, alternative credit scoring can expand financial inclusion while protecting the bank's capital. Getting this right means more approved loans, fewer defaults, and a competitive advantage in the BNPL market.

---

## ğŸ¯ Solution Overview

We transformed eCommerce transaction data into a predictive credit risk signal using **RFM (Recency, Frequency, Monetary) analysis** and **machine learning**. The solution creates a proxy for creditworthiness where no traditional credit data exists.

### How It Works
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Transactionâ”‚ â”‚ RFM Analysis â”‚ â”‚ Risk Scoring â”‚
â”‚ Data â”‚ â†’ â”‚ & Clustering â”‚ â†’ â”‚ & Prediction â”‚
â”‚ (eCommerce) â”‚ â”‚ (K-Means) â”‚ â”‚ (XGBoost) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Business Impact â”‚ â† â”‚ Interactive â”‚ â† â”‚ Explainable â”‚
â”‚ Calculator â”‚ â”‚ Dashboard â”‚ â”‚ AI (SHAP) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### Key Components

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Data Processing** | Python, Pandas, Scikit-learn | Transform raw transactions into RFM features |
| **Target Engineering** | K-Means Clustering | Create proxy default labels from customer behavior |
| **Model Training** | XGBoost, MLflow | Train and track multiple models with experiment tracking |
| **API** | FastAPI, Docker | Serve predictions in production |
| **Dashboard** | Streamlit, Plotly | Interactive visualization for stakeholders |
| **Explainability** | SHAP | Meet Basel II regulatory requirements |
| **CI/CD** | GitHub Actions | Automated testing and quality assurance |

---

## ğŸ“Š Key Results

### Model Performance
| Metric | Value | Interpretation |
|--------|-------|----------------|
| **ROC-AUC** | 0.91 | Excellent discrimination between risk levels |
| **F1 Score** | 0.84 | Strong balance of precision and recall |
| **Precision** | 0.87 | When we predict high risk, we're right 87% of the time |
| **Recall** | 0.81 | We catch 81% of actual high-risk customers |

### Business Impact (Projected)
| Metric | Value | Annual Impact |
|--------|-------|---------------|
| **Default Rate Reduction** | 35% | **$1.2M saved** in prevented losses |
| **Approval Rate Improvement** | 22% | **$3.4M additional loan volume** |
| **Manual Review Time** | 4 hrs â†’ 15 min | **$180K operational savings** |
| **Customer Acquisition** | +28% | Expanded market reach |

### Technical Excellence
| Metric | Achievement |
|--------|-------------|
| **Test Coverage** | 87% (17 unit tests) |
| **API Response Time** | <150ms (p95) |
| **CI/CD Status** | âœ… Always passing |
| **Code Quality** | 0 linting errors, full type hints |

---

## ğŸ® Demo

### Interactive Dashboard Preview

**Dashboard Features:**
- ğŸ“Š Real-time risk monitoring
- ğŸ‘¥ RFM customer segmentation
- ğŸ¤– Model performance metrics
- ğŸ”® Individual risk prediction
- ğŸ” SHAP explainability
- ğŸ’¼ Business impact calculator

### Live Demo
â–¶ï¸ **[View Live Dashboard](http://localhost:8501)** (run locally)

### API Demo
```bash
# Predict risk for a customer
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "customer_id": "CUST_12345",
    "recency": 15,
    "frequency": 8,
    "monetary": 250.0,
    "total_transaction_amount": 2000.0,
    "avg_transaction_amount": 250.0,
    "std_transaction_amount": 120.0
  }'

# Response
{
  "customer_id": "CUST_12345",
  "risk_probability": 0.32,
  "risk_category": "Low Risk",
  "prediction_timestamp": "2026-02-16T14:30:22Z"
}
```

---

## Quick Start
**Prerequisites**
- Python 3.9+
- Docker (optional, for containerized deployment)
- Git

### Installation
```bash
# 1. Clone the repository
git clone https://github.com/Jaki77/credit-risk-model.git
cd credit-risk-model

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run data processing and training
python src/data_processing.py
python src/train.py

# 5. Launch the dashboard
streamlit run src/dashboard/app.py

# 6. (Optional) Run the API
uvicorn src.api.main:app --reload --port 8000
```

### Docker Deployment
```bash
# Build and run with Docker Compose
docker-compose up --build

# Services:
# - API: http://localhost:8000
# - Dashboard: http://localhost:8501
```

### Run Tests
```bash
# Run all tests with coverage
pytest tests/ -v --cov=src/

# Run linting
flake8 src/ tests/
black --check src/ tests/
```

---

## Project Structure
credit-risk-model/
â”‚
â”œâ”€â”€ .github/workflows/          # CI/CD pipeline
â”‚   â””â”€â”€ ci.yml                  # GitHub Actions workflow
â”‚
â”œâ”€â”€ data/                       # (gitignored)
â”‚   â”œâ”€â”€ raw/                     # Raw transaction data
â”‚   â””â”€â”€ processed/                # Processed features
â”‚
â”œâ”€â”€ models/                      # Trained models
â”‚   â”œâ”€â”€ best_model.pkl           # Best performing model
â”‚   â”œâ”€â”€ feature_names.json        # Feature names
â”‚   â””â”€â”€ model_metrics.json        # Performance metrics
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb                # Exploratory Data Analysis
â”‚
â”œâ”€â”€ reports/                      # Generated reports
â”‚   â””â”€â”€ shap_report/              # SHAP explainability reports
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Dataclasses for configuration
â”‚   â”œâ”€â”€ data_processing.py          # RFM and feature engineering
â”‚   â”œâ”€â”€ train.py                    # Model training with MLflow
â”‚   â”œâ”€â”€ predict.py                  # Inference utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                         # FastAPI service
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                  # API endpoints
â”‚   â”‚   â””â”€â”€ pydantic_models.py        # Request/response schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ dashboard/                    # Streamlit dashboard
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py                     # Main dashboard
â”‚   â”‚   â”œâ”€â”€ components.py               # UI components
â”‚   â”‚   â”œâ”€â”€ utils.py                    # Helper functions
â”‚   â”‚   â””â”€â”€ shap_integration.py          # SHAP visualizations
â”‚   â”‚
â”‚   â””â”€â”€ explainability/                 # SHAP explainability
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ shap_explainer.py            # Core SHAP functionality
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_data_processing.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile                       # API containerization
â”œâ”€â”€ docker-compose.yml                # Multi-service setup
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ run_dashboard.py                    # Dashboard launcher
â”œâ”€â”€ generate_shap_report.py              # SHAP report generator
â””â”€â”€ README.md                           # You are here!

## Technical Deep Dive
## ğŸ”¬ Technical Deep Dive

### 1. Proxy Target Engineering (RFM + Clustering)

Since no historical default data exists, we created a proxy target using customer transaction patterns:

```python
@dataclass
class RFMConfig:
    n_clusters: int = 3
    random_state: int = 42
    snapshot_date: Optional[datetime] = None

def create_risk_proxy(transactions_df: pd.DataFrame) -> pd.DataFrame:
    """Create binary risk target using RFM and clustering"""
    
    # 1. Calculate RFM features
    rfm = transactions_df.groupby('CustomerId').agg({
        'TransactionStartTime': lambda x: (snapshot_date - x.max()).days,  # Recency
        'TransactionId': 'count',                                           # Frequency
        'Amount': 'sum'                                                     # Monetary
    }).rename(columns={
        'TransactionStartTime': 'Recency',
        'TransactionId': 'Frequency', 
        'Amount': 'Monetary'
    })
    
    # 2. Scale features
    scaler = StandardScaler()
    rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])
    
    # 3. Cluster customers
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(rfm_scaled)
    
    # 4. Identify high-risk cluster (lowest engagement)
    cluster_means = rfm.groupby(clusters).mean()
    high_risk_cluster = cluster_means['Frequency'].idxmin()  # Lowest frequency
    
    # 5. Create binary target
    rfm['is_high_risk'] = (clusters == high_risk_cluster).astype(int)
    
    return rfm
```
### 2. Model Training and Experiment Tracking
We used MLflow to track all experiments, comparing multiple algorithms:

| Model | ROC-AUC | F1 | Precision | Recall |
|-------|---------|----|-----------|--------|
| **XGBoost** | 0.91 | 0.84 | 0.87 | 0.81 |
| **Random Forest** | 0.89 | 0.82 | 0.88 | 0.77 |
| **Logistic Regression** | 0.83 | 0.75 | 0.79 | 0.71 |

Why XGBoost Won:

- Best handles non-linear relationships in RFM data
- Superior performance on imbalanced risk classes
- Built-in regularization prevents overfitting
- Feature importance for explainability

### 3. Production API with Fast API
```python
@app.post("/predict", response_model=PredictionResponse)
async def predict(customer_data: CustomerData):
    """
    Predict credit risk probability for a customer
    
    Args:
        customer_data: Customer features (recency, frequency, monetary, etc.)
    
    Returns:
        Risk probability and category
    """
    try:
        # Convert to DataFrame
        input_df = pd.DataFrame([customer_data.dict()])
        
        # Ensure correct feature order
        input_df = input_df[feature_names]
        
        # Predict probability
        probability = model.predict_proba(input_df)[0][1]
        
        # Determine risk category
        if probability < 0.3:
            category = "Low Risk"
        elif probability < 0.6:
            category = "Medium Risk"
        else:
            category = "High Risk"
        
        # Log prediction (for audit)
        logger.info(f"Prediction made for {customer_data.customer_id}: {probability:.3f}")
        
        return PredictionResponse(
            customer_id=customer_data.customer_id,
            risk_probability=round(probability, 4),
            risk_category=category,
            prediction_timestamp=datetime.utcnow().isoformat()
        )
    
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ” Model Explainability

**Why Explainability Matters in Finance**
Under **Basel II regulations**, banks must be able to explain credit decisions to:
- Customers who are denied credit
- Regulators auditing risk models
- Internal stakeholders making policy decisions

**Global Feature Importance**
Top factors driving credit risk:
1. Recency (32.5%) - Days since last transaction
2. Frequency (25.7%) - Number of transactions
3. Monetary (18.9%) - Total spending
4. Average Transaction Amount (12.6%)
5. Total Transaction Amount (6.3%)
6. Std Deviation of Amounts (4.0%)

## ğŸ’¼ Business Impact
### ROI Calculator
The dashboard includes an interactive ROI calculator that shows the model's financial impact:

| Scenario | Without Model	| With Model |	Improvement |
|----------|---------------|------------|-------------|
| Loan Volume | $10M | $12.2M | +22% |
| Default Rate | 5.0% | 3.25% | -35% |
| Losses | $500K | $325K | -$175K |
| Profit | $700K | $1.14M | +63% |

### Key Value Drivers
1. **Reduced Defaults**: Earlier identification of high-risk customers prevents losses
2. **Increased Approvals**: Low-risk customers get faster access to credit
3. **Operational Efficiency**: Automated decisions reduce manual review time
4. **Market Expansion**: Serve customers without traditional credit history

---

## ğŸ”® Future Improvements
Given more time, I would add:

### High Priority
- **Real-time data pipeline** with Kafka for streaming predictions
- **A/B testing framework** to compare model versions
- **Automated retraining** when performance degrades

### Medium Priority
- **Integration with core banking systems** via REST API
- **Mobile app** for loan officers in the field
- **Multi-currency support** for international transactions

### Low Priority
- **Alternative explainability** (LIME, Counterfactuals)
- **Deep learning models** (Transformers for sequence data)
- **Automated feature discovery** with genetic programming

---

## ğŸ‘¤ Author
Yakin Samuel
ğŸ“ AI Mastery Program, 10 Academy
ğŸ’¼ LinkedIn:  https://www.linkedin.com/in/yakin-samuel-8a288839a
ğŸ“§ Email: jakinsamuel1993@gmail.com